# -*- coding: utf-8 -*-
"""task1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kO9Mzmn2pY5W1XNszyVB0Is0cE0RVQQF
"""

!pip install transformers sentencepiece wikipedia googlesearch-python --quiet

import re
import random
from difflib import get_close_matches
import wikipedia
from googlesearch import search
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM


# 1️ Model Loader

class ModelLoader:
    def __init__(self, model_name="google/flan-t5-large"):
        print("Loading model...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
        print("Model loaded!")

    def generate(self, prompt, max_new_tokens=300, temperature=0.7, top_p=0.9):
        inputs = self.tokenizer(prompt, return_tensors="pt")
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=temperature,
            top_p=top_p
        )
        return self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()


# 2️ Multi-topic Memory

class MultiTopicMemory:
    def __init__(self, model, max_turns_per_topic=3):
        self.max_turns = max_turns_per_topic
        self.model = model
        self.topics = {}
        self.compact_memory = {}
        self.last_topic = None

    def add_turn(self, user_input, bot_response, topic="general"):
        if topic not in self.topics:
            self.topics[topic] = []
        self.topics[topic].append(f"User: {user_input}\nBot: {bot_response}")
        self.last_topic = topic
        if len(self.topics[topic]) > self.max_turns:
            self._summarize_topic(topic)
            self.topics[topic] = self.topics[topic][-self.max_turns:]

    def _summarize_topic(self, topic):
        old_context = "\n".join(self.topics[topic])
        prompt = f"Summarize this conversation briefly for topic '{topic}':\n{old_context}"
        try:
            summary = self.model.generate(prompt, max_new_tokens=120)
            self.compact_memory[topic] = summary
        except:
            self.compact_memory[topic] = "Summary unavailable."

    def get_context(self, topic="general"):
        recent = "\n".join(self.topics.get(topic, []))
        summary = self.compact_memory.get(topic, "")
        if summary:
            return f"{summary}\n{recent}"
        return recent

    def reset_topic(self):
        self.last_topic = None


# 3️ Knowledge Base

FACTS = {
    "python": "Python is a versatile programming language widely used for AI, machine learning, web development, and automation.",
    "ai": "Artificial Intelligence (AI) is the field of computer science focused on building machines that can perform tasks that typically require human intelligence.",
    "machine learning": "Machine Learning (ML) is a branch of AI that enables systems to learn and improve automatically from experience without being explicitly programmed.",
    "deep learning": "Deep Learning is a subfield of machine learning that uses neural networks with multiple layers to model complex patterns in data.",
    "computer vision": "Computer Vision enables computers to interpret and process visual information from the world, like images and videos."
}

CAPITALS = {
    "france": "Paris",
    "italy": "Rome",
    "germany": "Berlin",
    "spain": "Madrid",
    "india": "New Delhi",
    "usa": "Washington, D.C.",
    "japan": "Tokyo",
    "united kingdom": "London"
}

COUNTRY_ALIASES = {
    "america": "usa",
    "united states": "usa",
    "usa": "usa",
    "uk": "united kingdom",
    "britain": "united kingdom",
    "england": "united kingdom"
}

ALL_COUNTRIES = list(COUNTRY_ALIASES.keys()) + list(CAPITALS.keys())
ALL_FACTS = list(FACTS.keys())

GREETINGS = ["hello", "hi", "hey", "greetings"]
FUN_RESPONSES = [
    "Why did the computer go to therapy? Too many bytes of emotional data!",
    "I would tell you a joke about AI, but I’m still learning humor.",
    "Why did the AI cross the road? To optimize its loss function!"
]
PERSONALITY_RESPONSES = [
    "I’m feeling awesome — charged with data and ready to chat!",
    "Doing great! How about you?",
    "I’m running at full capacity today. No lag here!"
]

FOLLOW_UP_KEYS = ["and what about", "also", "what about", "tell me about"]
TOPIC_KEYWORDS = {
    "capital": ALL_COUNTRIES,
    "fact": list(FACTS.keys()),
    "greeting": GREETINGS,
    "fun": ["joke", "funny", "laugh"],
    "personal": ["how are you", "how's it going", "how do you feel"]
}


# 4️ Helper Functions

def detect_greeting(user_input):
    if any(word in user_input.lower() for word in GREETINGS):
        return random.choice([
            "Hello! How can I help you today?",
            "Hi there! What would you like to talk about?",
            "Hey! Ready to chat about tech or anything else?"
        ])
    return None

def random_fun_response():
    return random.choice(FUN_RESPONSES)

def random_personality_response():
    return random.choice(PERSONALITY_RESPONSES)

def detect_topic(user_input):
    user_lower = user_input.lower()
    for topic, keywords in TOPIC_KEYWORDS.items():
        for keyword in keywords:
            if keyword in user_lower:
                return topic
    return "general"

def fuzzy_country_lookup(user_input):
    matches = get_close_matches(user_input.lower(), [c.lower() for c in ALL_COUNTRIES], n=1, cutoff=0.7)
    if matches:
        match = matches[0]
        return COUNTRY_ALIASES.get(match) or match
    return None

def fuzzy_fact_lookup(user_input):
    matches = get_close_matches(user_input.lower(), ALL_FACTS, n=1, cutoff=0.7)
    if matches:
        return FACTS[matches[0]]
    return None

def wiki_summary(query):
    try:
        return wikipedia.summary(query, sentences=2)
    except wikipedia.exceptions.DisambiguationError as e:
        return f"{e.options[0]} (Ambiguous, showing one possible result)"
    except:
        return None

def google_snippet(query):
    try:
        results = list(search(query, num_results=3))
        if results:
            return f"Top Google result: {results[0]}"
        return None
    except:
        return None


# 5️ Query Handling

def query_fact(user_input, memory):
    user_input_clean = user_input.lower()
    match = re.search(r"capital of ([a-zA-Z\s]+)\??", user_input_clean)
    if match:
        country = match.group(1).strip()
        normalized = COUNTRY_ALIASES.get(country.lower()) or fuzzy_country_lookup(country)
        if normalized and normalized in CAPITALS:
            memory.last_topic = "capital"
            return f"The capital of {normalized.capitalize()} is {CAPITALS[normalized]}"
        else:
            return f"Sorry, I don't know the capital of {country}."
    for key, value in FACTS.items():
        if key in user_input_clean:
            memory.last_topic = "fact"
            return value
    fuzzy_fact = fuzzy_fact_lookup(user_input)
    if fuzzy_fact:
        memory.last_topic = "fact"
        return fuzzy_fact
    return None

def detect_followup(user_input, memory):
    user_input_clean = user_input.lower()
    for key in FOLLOW_UP_KEYS:
        if key in user_input_clean:
            entity = user_input_clean.split(key)[-1].strip().rstrip("?")
            normalized = COUNTRY_ALIASES.get(entity.lower()) or fuzzy_country_lookup(entity)
            if memory.last_topic == "capital" and normalized in CAPITALS:
                return f"The capital of {normalized.capitalize()} is {CAPITALS[normalized]}"
            elif memory.last_topic == "fact":
                fact_match = FACTS.get(entity.lower()) or fuzzy_fact_lookup(entity)
                if fact_match:
                    return fact_match
    return None


# 6️ Bot Response

def get_bot_response(user_input, model, memory):
    current_topic = detect_topic(user_input)
    context = memory.get_context(topic=current_topic)
    response = detect_greeting(user_input) or detect_followup(user_input, memory) or query_fact(user_input, memory)

    if not response:
        online_info = wiki_summary(user_input)
        if online_info:
            response = online_info
        else:
            google_info = google_snippet(user_input)
            if google_info:
                response = google_info
            else:
                prompt = f"Context:\n{context}\nUser asked: '{user_input}'"
                response = model.generate(prompt, max_new_tokens=300)

    memory.add_turn(user_input, response, topic=current_topic)
    return response


# 7️ CLI Loop

def run_chatbot():
    print("Welcome to Advanced Smart CLI Chatbot! Type '/exit' to quit.\n")
    model = ModelLoader()
    memory = MultiTopicMemory(model=model, max_turns_per_topic=3)

    while True:
        user_input = input("You: ").strip()
        if user_input.lower() == "/exit":
            print("Exiting chatbot. Goodbye!")
            break
        if not user_input:
            continue
        bot_response = get_bot_response(user_input, model, memory)
        print(f"Bot: {bot_response}\n")

run_chatbot()

